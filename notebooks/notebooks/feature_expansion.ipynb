{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a6c315",
   "metadata": {},
   "source": [
    "## Feature Expansion — Geo + Seasonality\n",
    "Goal: Add district/beat priors, lat/lon clusters, weekend/season/holiday flags. Then retrain HGB baseline and compare to 0.663 stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae26125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ART: /Volumes/easystore/Projects/chicago-crime-pipeline/notebooks/artifacts | stamp: 20250922-234027\n"
     ]
    }
   ],
   "source": [
    "import os, time, json, tempfile\n",
    "from pathlib import Path \n",
    "import numpy as np, pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, roc_auc_score, precision_recall_curve, roc_curve,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from scipy.stats import randint, loguniform\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths\n",
    "REPO = Path.cwd()\n",
    "while REPO.name != \"chicago-crime-pipeline\" and REPO.parent != REPO:\n",
    "    REPO = REPO.parent\n",
    "DATA = REPO / \"data\" / \"processed\"\n",
    "ART  = REPO / \"notebooks\" / \"artifacts\"\n",
    "ART.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "stamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(\"ART:\", ART, \"| stamp:\", stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58c62a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10482, 10)\n",
      "['id', 'date', 'primary_type', 'location_description', 'arrest', 'location_grouped', 'year', 'month', 'dow', 'hour']\n"
     ]
    }
   ],
   "source": [
    "# Load and split \n",
    "df = pd.read_csv(DATA / \"arrest_features.csv\")\n",
    "assert \"arrest\" in df.columns, df.columns\n",
    "\n",
    "# (If these columns exist, great. If not, the geo/season steps will gracefully skip.)\n",
    "print(\"shape:\", df.shape)\n",
    "print(df.columns.tolist()[:30])\n",
    "\n",
    "TARGET = \"arrest\"\n",
    "y = df[TARGET].astype(int).values\n",
    "X = df.drop(columns=[TARGET]).copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22ed218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make editable copies\n",
    "X_train_fe = X_train.copy()\n",
    "X_test_fe  = X_test.copy()\n",
    "\n",
    "# Weekday from date\n",
    "for Xdf in (X_train_fe, X_test_fe):\n",
    "    Xdf[\"weekday\"] = pd.to_datetime(Xdf[\"date\"]).dt.day_name()\n",
    "\n",
    "# Hour bins\n",
    "bins, labels = [0,6,12,18,24], [\"00-05\",\"06-11\",\"12-17\",\"18-23\"]\n",
    "for Xdf in (X_train_fe, X_test_fe):\n",
    "    Xdf[\"hour_bin\"] = pd.cut(Xdf[\"hour\"].astype(int), bins=bins, right=False, labels=labels)\n",
    "\n",
    "# Rare bucketing helper\n",
    "def rare_bucket(train_col, test_col, min_count=40):\n",
    "    vc = train_col.value_counts()\n",
    "    keep = set(vc[vc >= min_count].index)\n",
    "    return (train_col.where(train_col.isin(keep), \"__RARE__\"),\n",
    "            test_col.where(test_col.isin(keep),  \"__RARE__\"))\n",
    "\n",
    "for col in [\"location_description\",\"primary_type\"]:\n",
    "    X_train_fe[col], X_test_fe[col] = rare_bucket(X_train_fe[col], X_test_fe[col], 40)\n",
    "\n",
    "# Frequency encodings\n",
    "def add_freq(col):\n",
    "    freq = X_train_fe[col].astype(object).value_counts(normalize=True)\n",
    "    X_train_fe[f\"{col}_freq\"] = X_train_fe[col].map(freq).astype(float).fillna(0.0)\n",
    "    X_test_fe[f\"{col}_freq\"]  = X_test_fe[col].map(freq).astype(float).fillna(0.0)\n",
    "\n",
    "for c in [\"primary_type\",\"location_description\",\"weekday\",\"hour_bin\"]:\n",
    "    add_freq(c)\n",
    "\n",
    "# Target mean for primary_type (prior)\n",
    "ptype_rate = pd.Series(y_train).groupby(X_train_fe[\"primary_type\"]).mean()\n",
    "X_train_fe[\"ptype_arrest_rate\"] = X_train_fe[\"primary_type\"].map(ptype_rate)\n",
    "X_test_fe[\"ptype_arrest_rate\"]  = X_test_fe[\"primary_type\"].map(ptype_rate).fillna(float(ptype_rate.mean()))\n",
    "\n",
    "# Interaction ptype x hour_bin\n",
    "X_train_fe[\"ptype_x_hourbin\"] = X_train_fe[\"primary_type\"].astype(str) + \"_\" + X_train_fe[\"hour_bin\"].astype(str)\n",
    "X_test_fe[\"ptype_x_hourbin\"]  = X_test_fe[\"primary_type\"].astype(str)  + \"_\" + X_test_fe[\"hour_bin\"].astype(str)\n",
    "\n",
    "# Cyclical time\n",
    "for Xdf in (X_train_fe, X_test_fe):\n",
    "    Xdf[\"hour_sin\"]  = np.sin(2*np.pi * Xdf[\"hour\"].astype(float)/24.0)\n",
    "    Xdf[\"hour_cos\"]  = np.cos(2*np.pi * Xdf[\"hour\"].astype(float)/24.0)\n",
    "    Xdf[\"month_sin\"] = np.sin(2*np.pi * Xdf[\"month\"].astype(float)/12.0)\n",
    "    Xdf[\"month_cos\"] = np.cos(2*np.pi * Xdf[\"month\"].astype(float)/12.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d073c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[note] 'district' not found; skipping district priors.\n",
      "[note] 'beat' not found; skipping beat priors.\n",
      "[note] latitude/longitude not found; skipping KMeans geo_cluster.\n"
     ]
    }
   ],
   "source": [
    "# (A) District/beat priors if present \n",
    "geo_cols_present = set(X_train_fe.columns)\n",
    "\n",
    "if \"district\" in geo_cols_present:\n",
    "    dist_rate = pd.Series(y_train).groupby(X_train_fe[\"district\"]).mean()\n",
    "    X_train_fe[\"district_arrest_rate\"] = X_train_fe[\"district\"].map(dist_rate)\n",
    "    X_test_fe[\"district_arrest_rate\"]  = X_test_fe[\"district\"].map(dist_rate).fillna(float(dist_rate.mean()))\n",
    "else:\n",
    "    print(\"[note] 'district' not found; skipping district priors.\")\n",
    "\n",
    "if \"beat\" in geo_cols_present:\n",
    "    beat_rate = pd.Series(y_train).groupby(X_train_fe[\"beat\"]).mean()\n",
    "    X_train_fe[\"beat_arrest_rate\"] = X_train_fe[\"beat\"].map(beat_rate)\n",
    "    X_test_fe[\"beat_arrest_rate\"]  = X_test_fe[\"beat\"].map(beat_rate).fillna(float(beat_rate.mean()))\n",
    "else:\n",
    "    print(\"[note] 'beat' not found; skipping beat priors.\")\n",
    "\n",
    "# (B) Lat/Lon clustering → spatial buckets (if present)\n",
    "if {\"latitude\",\"longitude\"}.issubset(geo_cols_present):\n",
    "    # Use KMeans on train; same transform on test\n",
    "    k = 25  # tune later (e.g., 20–60)\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=\"auto\")\n",
    "    coords_train = X_train_fe[[\"latitude\",\"longitude\"]].astype(float).values\n",
    "    coords_test  = X_test_fe[[\"latitude\",\"longitude\"]].astype(float).values\n",
    "    X_train_fe[\"geo_cluster\"] = kmeans.fit_predict(coords_train).astype(str)\n",
    "    X_test_fe[\"geo_cluster\"]  = kmeans.predict(coords_test).astype(str)\n",
    "\n",
    "    # cluster priors\n",
    "    gc_rate = pd.Series(y_train).groupby(X_train_fe[\"geo_cluster\"]).mean()\n",
    "    X_train_fe[\"geo_cluster_rate\"] = X_train_fe[\"geo_cluster\"].map(gc_rate)\n",
    "    X_test_fe[\"geo_cluster_rate\"]  = X_test_fe[\"geo_cluster\"].map(gc_rate).fillna(float(gc_rate.mean()))\n",
    "else:\n",
    "    print(\"[note] latitude/longitude not found; skipping KMeans geo_cluster.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ac76a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New seasonal features\n",
    "\n",
    "# Weekend\n",
    "for Xdf in (X_train_fe, X_test_fe):\n",
    "    Xdf[\"is_weekend\"] = pd.to_datetime(Xdf[\"date\"]).dt.dayofweek.isin([5,6]).astype(int)\n",
    "\n",
    "# Summer (Jun-Aug)\n",
    "for Xdf in (X_train_fe, X_test_fe):\n",
    "    Xdf[\"is_summer\"] = Xdf[\"month\"].isin([6,7,8]).astype(int)\n",
    "\n",
    "# US Federal Holidays (approx; if date has no year, parse as datetime)\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "years = sorted(set(pd.to_datetime(X_train_fe[\"date\"]).dt.year) | set(pd.to_datetime(X_test_fe[\"date\"]).dt.year))\n",
    "holidays = cal.holidays(start=f\"{min(years)}-01-01\", end=f\"{max(years)}-12-31\")\n",
    "holidays = set(pd.to_datetime(holidays).date)\n",
    "\n",
    "def is_holiday(s):\n",
    "    d = pd.to_datetime(s).dt.date\n",
    "    return d.isin(holidays).astype(int)\n",
    "\n",
    "X_train_fe[\"is_holiday\"] = is_holiday(X_train_fe[\"date\"])\n",
    "X_test_fe[\"is_holiday\"]  = is_holiday(X_test_fe[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATS: ['date', 'primary_type', 'location_description', 'location_grouped', 'weekday', 'hour_bin', 'ptype_x_hourbin']\n",
      "NUMS: ['id', 'year', 'month', 'dow', 'hour', 'primary_type_freq', 'location_description_freq', 'weekday_freq', 'hour_bin_freq', 'ptype_arrest_rate', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'is_weekend', 'is_summer', 'is_holiday']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessor builder\n",
    "\n",
    "cat_base = [\n",
    "    \"date\",\"primary_type\",\"location_description\",\"location_grouped\",\n",
    "    \"weekday\",\"hour_bin\",\"ptype_x_hourbin\",\n",
    "    # geo buckets if exist\n",
    "    \"district\",\"beat\",\"geo_cluster\"\n",
    "]\n",
    "num_base = [\n",
    "    \"id\",\"year\",\"month\",\"dow\",\"hour\",\n",
    "    \"primary_type_freq\",\"location_description_freq\",\"weekday_freq\",\"hour_bin_freq\",\n",
    "    \"ptype_arrest_rate\",\n",
    "    # geo priors if exist\n",
    "    \"district_arrest_rate\",\"beat_arrest_rate\",\"geo_cluster_rate\",\n",
    "    # cyclical + seasonal\n",
    "    \"hour_sin\",\"hour_cos\",\"month_sin\",\"month_cos\",\n",
    "    \"is_weekend\",\"is_summer\",\"is_holiday\",\n",
    "    # raw coords if you want (can help trees a bit)\n",
    "    \"latitude\",\"longitude\"\n",
    "]\n",
    "\n",
    "def build_preprocessor(Xdf):\n",
    "    present = set(Xdf.columns)\n",
    "    cat_cols = [c for c in cat_base if c in present]\n",
    "    num_cols = [c for c in num_base if c in present]\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "            (\"num\", \"passthrough\", num_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "    return pre, cat_cols, num_cols\n",
    "\n",
    "pre_exp, cat_cols_exp, num_cols_exp = build_preprocessor(X_train_fe)\n",
    "print(\"CATS:\", cat_cols_exp)\n",
    "print(\"NUMS:\", num_cols_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a6fec77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] END clf__l2_regularization=0.0057537737941171915, clf__learning_rate=0.18214744423753768, clf__max_depth=6, clf__max_iter=271, clf__max_leaf_nodes=84, clf__min_samples_leaf=60; total time= 1.2min\n",
      "[CV] END clf__l2_regularization=0.0057537737941171915, clf__learning_rate=0.18214744423753768, clf__max_depth=6, clf__max_iter=271, clf__max_leaf_nodes=84, clf__min_samples_leaf=60; total time= 1.2min\n",
      "[CV] END clf__l2_regularization=0.0057537737941171915, clf__learning_rate=0.18214744423753768, clf__max_depth=6, clf__max_iter=271, clf__max_leaf_nodes=84, clf__min_samples_leaf=60; total time= 1.2min\n",
      "[CV] END clf__l2_regularization=0.0005409123677836541, clf__learning_rate=0.0403316977206044, clf__max_depth=6, clf__max_iter=287, clf__max_leaf_nodes=47, clf__min_samples_leaf=42; total time= 1.4min\n",
      "[CV] END clf__l2_regularization=0.0005409123677836541, clf__learning_rate=0.0403316977206044, clf__max_depth=6, clf__max_iter=287, clf__max_leaf_nodes=47, clf__min_samples_leaf=42; total time= 1.4min\n",
      "[CV] END clf__l2_regularization=0.0005409123677836541, clf__learning_rate=0.0403316977206044, clf__max_depth=6, clf__max_iter=287, clf__max_leaf_nodes=47, clf__min_samples_leaf=42; total time= 1.4min\n",
      "[CV] END clf__l2_regularization=4.5959188849801444, clf__learning_rate=0.09679873188879931, clf__max_depth=5, clf__max_iter=221, clf__max_leaf_nodes=72, clf__min_samples_leaf=130; total time=  52.9s\n",
      "[CV] END clf__l2_regularization=4.5959188849801444, clf__learning_rate=0.09679873188879931, clf__max_depth=5, clf__max_iter=221, clf__max_leaf_nodes=72, clf__min_samples_leaf=130; total time=  53.5s\n",
      "[CV] END clf__l2_regularization=4.5959188849801444, clf__learning_rate=0.09679873188879931, clf__max_depth=5, clf__max_iter=221, clf__max_leaf_nodes=72, clf__min_samples_leaf=130; total time=  52.1s\n",
      "[CV] END clf__l2_regularization=0.00012494702536130708, clf__learning_rate=0.1889028280863132, clf__max_depth=7, clf__max_iter=493, clf__max_leaf_nodes=25, clf__min_samples_leaf=103; total time= 2.4min\n",
      "[CV] END clf__l2_regularization=0.00012494702536130708, clf__learning_rate=0.1889028280863132, clf__max_depth=7, clf__max_iter=493, clf__max_leaf_nodes=25, clf__min_samples_leaf=103; total time= 2.5min\n",
      "[CV] END clf__l2_regularization=0.07500295933641415, clf__learning_rate=0.039088721491346144, clf__max_depth=7, clf__max_iter=387, clf__max_leaf_nodes=38, clf__min_samples_leaf=101; total time= 1.8min\n",
      "[CV] END clf__l2_regularization=0.07500295933641415, clf__learning_rate=0.039088721491346144, clf__max_depth=7, clf__max_iter=387, clf__max_leaf_nodes=38, clf__min_samples_leaf=101; total time= 1.8min\n",
      "[CV] END clf__l2_regularization=0.00026653663965080183, clf__learning_rate=0.09696497572904088, clf__max_depth=7, clf__max_iter=254, clf__max_leaf_nodes=87, clf__min_samples_leaf=42; total time= 1.3min\n",
      "[CV] END clf__l2_regularization=0.00026653663965080183, clf__learning_rate=0.09696497572904088, clf__max_depth=7, clf__max_iter=254, clf__max_leaf_nodes=87, clf__min_samples_leaf=42; total time= 1.3min\n",
      "[CV] END clf__l2_regularization=0.00012494702536130708, clf__learning_rate=0.1889028280863132, clf__max_depth=7, clf__max_iter=493, clf__max_leaf_nodes=25, clf__min_samples_leaf=103; total time= 2.4min\n",
      "[CV] END clf__l2_regularization=1.0985862948198042, clf__learning_rate=0.10905173569046274, clf__max_depth=4, clf__max_iter=366, clf__max_leaf_nodes=41, clf__min_samples_leaf=43; total time= 1.3min\n",
      "[CV] END clf__l2_regularization=0.00026653663965080183, clf__learning_rate=0.09696497572904088, clf__max_depth=7, clf__max_iter=254, clf__max_leaf_nodes=87, clf__min_samples_leaf=42; total time= 1.3min\n",
      "[CV] END clf__l2_regularization=0.07500295933641415, clf__learning_rate=0.039088721491346144, clf__max_depth=7, clf__max_iter=387, clf__max_leaf_nodes=38, clf__min_samples_leaf=101; total time= 1.9min\n",
      "[CV] END clf__l2_regularization=1.0985862948198042, clf__learning_rate=0.10905173569046274, clf__max_depth=4, clf__max_iter=366, clf__max_leaf_nodes=41, clf__min_samples_leaf=43; total time= 1.3min\n",
      "[CV] END clf__l2_regularization=1.0985862948198042, clf__learning_rate=0.10905173569046274, clf__max_depth=4, clf__max_iter=366, clf__max_leaf_nodes=41, clf__min_samples_leaf=43; total time= 1.3min\n",
      "[CV] END clf__l2_regularization=2.6753326391898704, clf__learning_rate=0.08734123752313651, clf__max_depth=5, clf__max_iter=464, clf__max_leaf_nodes=76, clf__min_samples_leaf=41; total time= 1.7min\n",
      "[CV] END clf__l2_regularization=2.6753326391898704, clf__learning_rate=0.08734123752313651, clf__max_depth=5, clf__max_iter=464, clf__max_leaf_nodes=76, clf__min_samples_leaf=41; total time= 1.8min\n",
      "[CV] END clf__l2_regularization=1.8744302901656154, clf__learning_rate=0.049015376570470164, clf__max_depth=7, clf__max_iter=201, clf__max_leaf_nodes=29, clf__min_samples_leaf=93; total time=  56.6s\n",
      "[CV] END clf__l2_regularization=2.6753326391898704, clf__learning_rate=0.08734123752313651, clf__max_depth=5, clf__max_iter=464, clf__max_leaf_nodes=76, clf__min_samples_leaf=41; total time= 1.8min\n",
      "[CV] END clf__l2_regularization=1.8744302901656154, clf__learning_rate=0.049015376570470164, clf__max_depth=7, clf__max_iter=201, clf__max_leaf_nodes=29, clf__min_samples_leaf=93; total time=  58.6s\n",
      "[CV] END clf__l2_regularization=0.1641309440776004, clf__learning_rate=0.06914594902276397, clf__max_depth=7, clf__max_iter=463, clf__max_leaf_nodes=70, clf__min_samples_leaf=74; total time= 2.3min\n",
      "[CV] END clf__l2_regularization=0.1641309440776004, clf__learning_rate=0.06914594902276397, clf__max_depth=7, clf__max_iter=463, clf__max_leaf_nodes=70, clf__min_samples_leaf=74; total time= 2.2min\n",
      "[CV] END clf__l2_regularization=1.8744302901656154, clf__learning_rate=0.049015376570470164, clf__max_depth=7, clf__max_iter=201, clf__max_leaf_nodes=29, clf__min_samples_leaf=93; total time= 1.0min\n",
      "[CV] END clf__l2_regularization=0.1641309440776004, clf__learning_rate=0.06914594902276397, clf__max_depth=7, clf__max_iter=463, clf__max_leaf_nodes=70, clf__min_samples_leaf=74; total time= 2.2min\n",
      "[CV] END clf__l2_regularization=0.03706595581487585, clf__learning_rate=0.042601457381983523, clf__max_depth=5, clf__max_iter=417, clf__max_leaf_nodes=67, clf__min_samples_leaf=73; total time= 1.5min\n",
      "[CV] END clf__l2_regularization=0.03706595581487585, clf__learning_rate=0.042601457381983523, clf__max_depth=5, clf__max_iter=417, clf__max_leaf_nodes=67, clf__min_samples_leaf=73; total time= 1.4min\n",
      "[CV] END clf__l2_regularization=0.03706595581487585, clf__learning_rate=0.042601457381983523, clf__max_depth=5, clf__max_iter=417, clf__max_leaf_nodes=67, clf__min_samples_leaf=73; total time= 1.5min\n",
      "[CV] END clf__l2_regularization=0.007191162557683139, clf__learning_rate=0.17402177501189842, clf__max_depth=7, clf__max_iter=389, clf__max_leaf_nodes=63, clf__min_samples_leaf=124; total time= 1.6min\n",
      "[CV] END clf__l2_regularization=0.007191162557683139, clf__learning_rate=0.17402177501189842, clf__max_depth=7, clf__max_iter=389, clf__max_leaf_nodes=63, clf__min_samples_leaf=124; total time= 1.3min\n",
      "[CV] END clf__l2_regularization=0.007191162557683139, clf__learning_rate=0.17402177501189842, clf__max_depth=7, clf__max_iter=389, clf__max_leaf_nodes=63, clf__min_samples_leaf=124; total time= 1.4min\n",
      "Best params: {'clf__l2_regularization': np.float64(0.07500295933641415), 'clf__learning_rate': np.float64(0.039088721491346144), 'clf__max_depth': 7, 'clf__max_iter': 387, 'clf__max_leaf_nodes': 38, 'clf__min_samples_leaf': 101} | CV AP: 0.6279\n",
      "\n",
      "=== TEST metrics (HGB + GEO/SEASON) ===\n",
      "PR-AUC: 0.6577\n",
      "ROC-AUC: 0.8892\n",
      "Best threshold: 0.7219124506924276 Best F1: 0.6189624329154245\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.930     0.953     0.941      1795\n",
      "           1      0.673     0.573     0.619       302\n",
      "\n",
      "    accuracy                          0.898      2097\n",
      "   macro avg      0.802     0.763     0.780      2097\n",
      "weighted avg      0.893     0.898     0.895      2097\n",
      "\n",
      "Confusion:\n",
      " [[1711   84]\n",
      " [ 129  173]]\n"
     ]
    }
   ],
   "source": [
    "# Quick HGB baseline on expanded features\n",
    "\n",
    "pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
    "sw_train = np.where(y_train==1, pos_weight, 1.0)\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    random_state=42, max_bins=255\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    \"clf__learning_rate\": loguniform(0.03, 0.2),\n",
    "    \"clf__max_depth\": randint(4, 9),\n",
    "    \"clf__max_leaf_nodes\": randint(24, 96),\n",
    "    \"clf__min_samples_leaf\": randint(40, 160),\n",
    "    \"clf__l2_regularization\": loguniform(1e-4, 5.0),\n",
    "    \"clf__max_iter\": randint(200, 500),\n",
    "}\n",
    "\n",
    "pipe = Pipeline([(\"pre\", pre_exp), (\"clf\", hgb)], memory=tempfile.mkdtemp())\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# quick n_iter so it finishes fast; bump to 20–30 later\n",
    "search = RandomizedSearchCV(\n",
    "    pipe, param_distributions=param_dist, n_iter=12,\n",
    "    scoring=\"average_precision\", refit=True, cv=cv, n_jobs=-1, random_state=42, verbose=2\n",
    ")\n",
    "search.fit(X_train_fe, y_train, clf__sample_weight=sw_train)\n",
    "print(\"Best params:\", search.best_params_, \"| CV AP:\", round(search.best_score_, 4))\n",
    "\n",
    "final = search.best_estimator_.fit(X_train_fe, y_train, clf__sample_weight=sw_train)\n",
    "proba = final.predict_proba(X_test_fe)[:,1]\n",
    "\n",
    "ap  = average_precision_score(y_test, proba)\n",
    "roc = roc_auc_score(y_test, proba)\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_test, proba)\n",
    "f1s = 2*prec*rec/(prec+rec+1e-12)\n",
    "i = np.nanargmax(f1s)\n",
    "thr_best = thr[i] if i < len(thr) else 0.5\n",
    "pred = (proba >= thr_best).astype(int)\n",
    "\n",
    "print(\"\\n=== TEST metrics (HGB + GEO/SEASON) ===\")\n",
    "print(\"PR-AUC:\", round(ap, 4))\n",
    "print(\"ROC-AUC:\", round(roc, 4))\n",
    "print(\"Best threshold:\", float(thr_best), \"Best F1:\", float(f1s[i]))\n",
    "print(classification_report(y_test, pred, digits=3))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "761154a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifacts → /Volumes/easystore/Projects/chicago-crime-pipeline/notebooks/artifacts\n"
     ]
    }
   ],
   "source": [
    "meta = {\n",
    "    \"timestamp\": stamp,\n",
    "    \"approach\": \"HGB + geo/season expansion\",\n",
    "    \"test_pr_auc\": float(ap),\n",
    "    \"test_roc_auc\": float(roc),\n",
    "    \"best_threshold\": float(thr_best),\n",
    "    \"best_params\": {k: (float(v) if hasattr(v, \"item\") else v) for k,v in search.best_params_.items()}\n",
    "}\n",
    "with open(ART / f\"hgb_geo_season_meta_{stamp}.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "prec_, rec_, _ = precision_recall_curve(y_test, proba)\n",
    "fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "\n",
    "plt.figure(); plt.plot(rec_, prec_); plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.grid(True, alpha=0.3)\n",
    "plt.title(f\"HGB + GEO/SEASON PR (AP={ap:.3f})\")\n",
    "plt.savefig(ART / f\"pr_curve_hgb_geo_{stamp}.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "plt.figure(); plt.plot(fpr, tpr); plt.plot([0,1],[0,1],'--')\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.grid(True, alpha=0.3)\n",
    "plt.title(f\"HGB + GEO/SEASON ROC (AUC={roc:.3f})\")\n",
    "plt.savefig(ART / f\"roc_curve_hgb_geo_{stamp}.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "with open(ART / f\"decision_threshold_hgb_geo_{stamp}.txt\", \"w\") as f:\n",
    "    f.write(str(thr_best))\n",
    "\n",
    "print(\"Saved artifacts →\", ART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "731b7c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Volumes/easystore/Projects/chicago-crime-pipeline/notebooks/artifacts/slice_metrics_primary_type_hgb_geo_20250922-234027.csv\n",
      "Saved: /Volumes/easystore/Projects/chicago-crime-pipeline/notebooks/artifacts/slice_metrics_weekday_hgb_geo_20250922-234027.csv\n",
      "Saved: /Volumes/easystore/Projects/chicago-crime-pipeline/notebooks/artifacts/slice_metrics_hour_bin_hgb_geo_20250922-234027.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6z/l9wv3crd4n5bzgcrdd7vxq8m0000gn/T/ipykernel_14455/535738392.py:6: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for val,g in d.groupby(slice_col):\n"
     ]
    }
   ],
   "source": [
    "def slice_metrics(X_df, y_true, proba, threshold, slice_col, min_support=40):\n",
    "    if slice_col not in X_df.columns: return None\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    d = pd.DataFrame({slice_col: X_df[slice_col], \"y\": y_true, \"pred\": (proba >= threshold).astype(int)})\n",
    "    rows=[]\n",
    "    for val,g in d.groupby(slice_col):\n",
    "        if len(g)<min_support: continue\n",
    "        p,r,f1,_ = precision_recall_fscore_support(g[\"y\"], g[\"pred\"], average=\"binary\", zero_division=0)\n",
    "        rows.append({slice_col: val, \"support\": int(len(g)), \"precision\": float(p), \"recall\": float(r), \"f1\": float(f1)})\n",
    "    return pd.DataFrame(rows).sort_values(\"f1\", ascending=False) if rows else None\n",
    "\n",
    "for col in [\"primary_type\",\"weekday\",\"hour_bin\",\"district\",\"beat\",\"geo_cluster\"]:\n",
    "    tbl = slice_metrics(X_test_fe, y_test, proba, thr_best, col, min_support=40)\n",
    "    if tbl is not None:\n",
    "        out = ART / f\"slice_metrics_{col}_hgb_geo_{stamp}.csv\"\n",
    "        tbl.to_csv(out, index=False); print(\"Saved:\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c4df1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF (geo/season) PR-AUC: 0.6479 | ROC-AUC: 0.8739\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_geo = Pipeline(steps=[\n",
    "    (\"pre\", pre_exp),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=12,\n",
    "        min_samples_split=6,\n",
    "        min_samples_leaf=6,\n",
    "        max_features=0.6,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        bootstrap=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_geo.fit(X_train_fe, y_train)\n",
    "proba_rf_geo = rf_geo.predict_proba(X_test_fe)[:,1]\n",
    "ap_rf_geo = average_precision_score(y_test, proba_rf_geo)\n",
    "roc_rf_geo = roc_auc_score(y_test, proba_rf_geo)\n",
    "print(\"RF (geo/season) PR-AUC:\", round(ap_rf_geo,4), \"| ROC-AUC:\", round(roc_rf_geo,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "519500fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST metrics (STACK RF+HGB on geo/season) ===\n",
      "PR-AUC: 0.6601\n",
      "ROC-AUC: 0.881\n",
      "Best threshold: 0.7695998051555637 Best F1: 0.6204379562038848\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.929     0.958     0.943      1795\n",
      "           1      0.691     0.563     0.620       302\n",
      "\n",
      "    accuracy                          0.901      2097\n",
      "   macro avg      0.810     0.760     0.782      2097\n",
      "weighted avg      0.894     0.901     0.897      2097\n",
      "\n",
      "Confusion:\n",
      " [[1719   76]\n",
      " [ 132  170]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "oof_rf = np.zeros(len(y_train)); oof_hgb = np.zeros(len(y_train))\n",
    "\n",
    "for fold,(tr,val) in enumerate(cv.split(X_train_fe, y_train),1):\n",
    "    Xtr, Xval = X_train_fe.iloc[tr], X_train_fe.iloc[val]\n",
    "    ytr, yval = y_train[tr], y_train[val]\n",
    "\n",
    "    rf_f  = Pipeline([(\"pre\", pre_exp), (\"clf\", rf_geo.named_steps[\"clf\"].__class__(**rf_geo.named_steps[\"clf\"].get_params()))])\n",
    "    hgb_f = Pipeline([(\"pre\", pre_exp), (\"clf\", final.named_steps[\"clf\"].__class__(**final.named_steps[\"clf\"].get_params()))])\n",
    "\n",
    "    rf_f.fit(Xtr, ytr)\n",
    "    hgb_f.fit(Xtr, ytr)\n",
    "\n",
    "    oof_rf[val]  = rf_f.predict_proba(Xval)[:,1]\n",
    "    oof_hgb[val] = hgb_f.predict_proba(Xval)[:,1]\n",
    "\n",
    "X_meta_geo = np.column_stack([oof_rf, oof_hgb])\n",
    "meta_geo = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "meta_geo.fit(X_meta_geo, y_train)\n",
    "\n",
    "# fit full bases on all train\n",
    "rf_full_geo  = Pipeline([(\"pre\", pre_exp), (\"clf\", rf_geo.named_steps[\"clf\"].__class__(**rf_geo.named_steps[\"clf\"].get_params()))]).fit(X_train_fe, y_train)\n",
    "hgb_full_geo = Pipeline([(\"pre\", pre_exp), (\"clf\", final.named_steps[\"clf\"].__class__(**final.named_steps[\"clf\"].get_params()))]).fit(X_train_fe, y_train)\n",
    "\n",
    "# test stack\n",
    "rf_test_geo  = rf_full_geo.predict_proba(X_test_fe)[:,1]\n",
    "hgb_test_geo = hgb_full_geo.predict_proba(X_test_fe)[:,1]\n",
    "proba_stack_geo = meta_geo.predict_proba(np.column_stack([rf_test_geo, hgb_test_geo]))[:,1]\n",
    "\n",
    "ap_stack  = average_precision_score(y_test, proba_stack_geo)\n",
    "roc_stack = roc_auc_score(y_test, proba_stack_geo)\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_test, proba_stack_geo)\n",
    "f1s = 2*prec*rec/(prec+rec+1e-12)\n",
    "i = np.nanargmax(f1s)\n",
    "thr_stack_geo = thr[i] if i < len(thr) else 0.5\n",
    "pred_stack_geo = (proba_stack_geo >= thr_stack_geo).astype(int)\n",
    "\n",
    "print(\"\\n=== TEST metrics (STACK RF+HGB on geo/season) ===\")\n",
    "print(\"PR-AUC:\", round(ap_stack,4))\n",
    "print(\"ROC-AUC:\", round(roc_stack,4))\n",
    "print(\"Best threshold:\", float(thr_stack_geo), \"Best F1:\", float(f1s[i]))\n",
    "print(classification_report(y_test, pred_stack_geo, digits=3))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_test, pred_stack_geo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f51f84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved stack models → /Volumes/easystore/Projects/chicago-crime-pipeline/notebooks/artifacts/stack_geo_models_20250922-234027.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "stack_geo_meta = {\n",
    "    \"timestamp\": stamp,\n",
    "    \"approach\": \"STACK RF+HGB on geo/season features\",\n",
    "    \"test_pr_auc\": float(ap_stack),\n",
    "    \"test_roc_auc\": float(roc_stack),\n",
    "    \"best_threshold\": float(thr_stack_geo),\n",
    "}\n",
    "with open(ART / f\"stack_geo_meta_{stamp}.json\", \"w\") as f:\n",
    "    json.dump(stack_geo_meta, f, indent=2)\n",
    "\n",
    "with open(ART / f\"decision_threshold_stack_geo_{stamp}.txt\", \"w\") as f:\n",
    "    f.write(str(thr_stack_geo))\n",
    "\n",
    "# Save full objects so you can load in prod\n",
    "joblib.dump({\"rf\": rf_full_geo, \"hgb\": hgb_full_geo, \"meta\": meta_geo}, ART / f\"stack_geo_models_{stamp}.pkl\")\n",
    "print(\"Saved stack models →\", ART / f\"stack_geo_models_{stamp}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
