{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3177a2",
   "metadata": {},
   "source": [
    "# Arrest Prediction — Segmented Models (HGB per crime_group)\n",
    "\n",
    "Goal: Beat the global HGB (~0.649 PR-AUC) by training **separate HGB models**\n",
    "for each `crime_group` = {violent, property, other}, then blending predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f90d20bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10482, 10) {0: 8972, 1: 1510}\n"
     ]
    }
   ],
   "source": [
    "# Core\n",
    "import os, time, json, tempfile\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modeling\n",
    "from scipy.stats import loguniform, randint\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, roc_auc_score, precision_recall_curve,\n",
    "    roc_curve, classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths \n",
    "REPO = Path.cwd()\n",
    "while REPO.name != \"chicago-crime-pipeline\" and REPO.parent != REPO:\n",
    "    REPO = REPO.parent\n",
    "DATA = REPO / \"data\" / \"processed\"\n",
    "ART = REPO / \"notebooks\" / \"artifacts\"\n",
    "ART.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load processed features\n",
    "df = pd.read_csv(DATA / \"arrest_features.csv\")\n",
    "assert \"arrest\" in df.columns\n",
    "print(df.shape, df['arrest'].value_counts().to_dict())\n",
    "\n",
    "# Split \n",
    "TARGET = \"arrest\"\n",
    "y = df[TARGET].astype(int).values\n",
    "X = df.drop(columns=[TARGET]).copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2a7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice metrics helper \n",
    "def slice_metrics(X_df, y_true, proba, threshold, slice_col, min_support=40):\n",
    "    if slice_col not in X_df.columns:\n",
    "        print(f\"[skip] no column: {slice_col}\")\n",
    "        return None\n",
    "    zz = pd.DataFrame({\n",
    "        slice_col: X_df[slice_col],\n",
    "        \"y\": y_true,\n",
    "        \"pred\": (proba >= threshold).astype(int)\n",
    "    })\n",
    "    rows = []\n",
    "    for val, g in zz.groupby(slice_col):\n",
    "        n = len(g)\n",
    "        if n < min_support: \n",
    "            continue\n",
    "        p, r, f1, _ = precision_recall_fscore_support(\n",
    "            g[\"y\"], g[\"pred\"], average=\"binary\", zero_division=0\n",
    "        )\n",
    "        rows.append({slice_col: val, \"support\": int(n),\n",
    "                     \"precision\": float(p), \"recall\": float(r), \"f1\": float(f1)})\n",
    "    if not rows:\n",
    "        print(f\"[note] no slices ≥{min_support} for {slice_col}\")\n",
    "        return None\n",
    "    return pd.DataFrame(rows).sort_values(\"f1\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "922543d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE done. Train/Test: (8385, 24) (2097, 24)\n"
     ]
    }
   ],
   "source": [
    "# Work on copies\n",
    "X_train_fe = X_train.copy()\n",
    "X_test_fe  = X_test.copy()\n",
    "\n",
    "# Weekday\n",
    "for Xdf in (X_train_fe, X_test_fe):\n",
    "    Xdf[\"weekday\"] = pd.to_datetime(Xdf[\"date\"]).dt.day_name()\n",
    "\n",
    "# Hour bins\n",
    "bins, labels = [0,6,12,18,24], [\"00-05\",\"06-11\",\"12-17\",\"18-23\"]\n",
    "for Xdf in (X_train_fe, X_test_fe):\n",
    "    Xdf[\"hour_bin\"] = pd.cut(Xdf[\"hour\"].astype(int), bins=bins, right=False, labels=labels)\n",
    "\n",
    "# Rare bucket helper\n",
    "def rare_bucket(train_col, test_col, min_count=40):\n",
    "    vc = train_col.value_counts()\n",
    "    keep = set(vc[vc >= min_count].index)\n",
    "    return (\n",
    "        train_col.where(train_col.isin(keep), \"__RARE__\"),\n",
    "        test_col.where(test_col.isin(keep), \"__RARE__\")\n",
    "    )\n",
    "\n",
    "# Rare-bucket high-card cats\n",
    "for col in [\"location_description\", \"primary_type\"]:\n",
    "    X_train_fe[col], X_test_fe[col] = rare_bucket(X_train_fe[col], X_test_fe[col], 40)\n",
    "\n",
    "# Frequency encodings\n",
    "def add_freq(col):\n",
    "    freq = X_train_fe[col].astype(object).value_counts(normalize=True)\n",
    "    X_train_fe[f\"{col}_freq\"] = X_train_fe[col].map(freq).astype(float).fillna(0.0)\n",
    "    X_test_fe[f\"{col}_freq\"]  = X_test_fe[col].map(freq).astype(float).fillna(0.0)\n",
    "for col in [\"primary_type\",\"location_description\",\"weekday\",\"hour_bin\"]:\n",
    "    add_freq(col)\n",
    "\n",
    "# Target mean prior for primary_type\n",
    "ptype_rate = pd.Series(y_train, index=X_train_fe.index).groupby(X_train_fe[\"primary_type\"]).mean()\n",
    "X_train_fe[\"ptype_arrest_rate\"] = X_train_fe[\"primary_type\"].map(ptype_rate).astype(float)\n",
    "X_test_fe[\"ptype_arrest_rate\"]  = X_test_fe[\"primary_type\"].map(ptype_rate).fillna(ptype_rate.mean()).astype(float)\n",
    "\n",
    "# Interaction: primary_type × hour_bin\n",
    "X_train_fe[\"ptype_x_hourbin\"] = X_train_fe[\"primary_type\"].astype(str) + \"_\" + X_train_fe[\"hour_bin\"].astype(str)\n",
    "X_test_fe[\"ptype_x_hourbin\"]  = X_test_fe[\"primary_type\"].astype(str)  + \"_\" + X_test_fe[\"hour_bin\"].astype(str)\n",
    "X_train_fe[\"ptype_x_hourbin\"], X_test_fe[\"ptype_x_hourbin\"] = rare_bucket(\n",
    "    X_train_fe[\"ptype_x_hourbin\"], X_test_fe[\"ptype_x_hourbin\"], min_count=30\n",
    ")\n",
    "\n",
    "# Segmentation label (feature-level, used only to route)\n",
    "violent   = {\"ASSAULT\",\"BATTERY\",\"ROBBERY\",\"WEAPONS VIOLATION\"}\n",
    "property_ = {\"BURGLARY\",\"THEFT\",\"MOTOR VEHICLE THEFT\",\"CRIMINAL DAMAGE\"}\n",
    "grp_map = {**{v:\"violent\" for v in violent}, **{p:\"property\" for p in property_}}\n",
    "for Xdf in (X_train_fe, X_test_fe):\n",
    "    Xdf[\"crime_group\"] = Xdf[\"primary_type\"].map(grp_map).fillna(\"other\")\n",
    "\n",
    "# Group prior\n",
    "y_train_s = pd.Series(y_train, index=X_train_fe.index)\n",
    "cg_rate = y_train_s.groupby(X_train_fe[\"crime_group\"]).mean()\n",
    "X_train_fe[\"crime_group_arrest_rate\"] = X_train_fe[\"crime_group\"].map(cg_rate).astype(float)\n",
    "X_test_fe[\"crime_group_arrest_rate\"]  = X_test_fe[\"crime_group\"].map(cg_rate).fillna(cg_rate.mean()).astype(float)\n",
    "\n",
    "# Interaction: crime_group × hour_bin\n",
    "X_train_fe[\"cg_x_hourbin\"] = X_train_fe[\"crime_group\"].astype(str) + \"_\" + X_train_fe[\"hour_bin\"].astype(str)\n",
    "X_test_fe[\"cg_x_hourbin\"]  = X_test_fe[\"crime_group\"].astype(str)  + \"_\" + X_test_fe[\"hour_bin\"].astype(str)\n",
    "X_train_fe[\"cg_x_hourbin\"], X_test_fe[\"cg_x_hourbin\"] = rare_bucket(\n",
    "    X_train_fe[\"cg_x_hourbin\"], X_test_fe[\"cg_x_hourbin\"], 30\n",
    ")\n",
    "\n",
    "# Cyclical time features\n",
    "for Xdf in (X_train_fe, X_test_fe):\n",
    "    Xdf[\"hour_sin\"]  = np.sin(2*np.pi * Xdf[\"hour\"].astype(float)/24.0)\n",
    "    Xdf[\"hour_cos\"]  = np.cos(2*np.pi * Xdf[\"hour\"].astype(float)/24.0)\n",
    "    Xdf[\"month_sin\"] = np.sin(2*np.pi * Xdf[\"month\"].astype(float)/12.0)\n",
    "    Xdf[\"month_cos\"] = np.cos(2*np.pi * Xdf[\"month\"].astype(float)/12.0)\n",
    "\n",
    "print(\"FE done. Train/Test:\", X_train_fe.shape, X_test_fe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12eb42f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base column lists\n",
    "cat_base = [\n",
    "    \"date\",\"primary_type\",\"location_description\",\"location_grouped\",\n",
    "    \"weekday\",\"hour_bin\",\"ptype_x_hourbin\",\"crime_group\",\"cg_x_hourbin\"\n",
    "]\n",
    "num_base = [\n",
    "    \"id\",\"year\",\"month\",\"dow\",\"hour\",\n",
    "    \"primary_type_freq\",\"location_description_freq\",\"weekday_freq\",\"hour_bin_freq\",\n",
    "    \"ptype_arrest_rate\",\"crime_group_arrest_rate\",\n",
    "    \"hour_sin\",\"hour_cos\",\"month_sin\",\"month_cos\"\n",
    "]\n",
    "\n",
    "def build_preprocessor(Xdf):\n",
    "    present = set(Xdf.columns)\n",
    "    cat_cols = [c for c in cat_base if c in present]\n",
    "    num_cols = [c for c in num_base if c in present]\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "            (\"num\", \"passthrough\", num_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "    return pre, cat_cols, num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27737eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_group_model(group_name, Xtr, ytr, n_iter=8, folds=3, seed=42):\n",
    "    pre, cat_cols, num_cols = build_preprocessor(Xtr)\n",
    "\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"pre\", pre),\n",
    "        (\"clf\", HistGradientBoostingClassifier(random_state=seed, max_bins=255))\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "        \"clf__learning_rate\": loguniform(0.01, 0.3),\n",
    "        \"clf__max_depth\": randint(3, 9),\n",
    "        \"clf__max_leaf_nodes\": randint(32, 128),\n",
    "        \"clf__min_samples_leaf\": randint(20, 200),\n",
    "        \"clf__l2_regularization\": loguniform(1e-4, 1.0),\n",
    "        \"clf__max_iter\": randint(150, 300),\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    search = RandomizedSearchCV(\n",
    "        pipe, param_distributions=param_dist,\n",
    "        n_iter=n_iter, scoring=\"average_precision\",\n",
    "        refit=True, cv=cv, n_jobs=-1, random_state=seed, verbose=1\n",
    "    )\n",
    "    search.fit(Xtr, ytr)\n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "effb2789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training group: violent | rows=2798 pos=482 ===\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best params: {'clf__l2_regularization': np.float64(0.05262961031076742), 'clf__learning_rate': np.float64(0.04628518674713462), 'clf__max_depth': 4, 'clf__max_iter': 281, 'clf__max_leaf_nodes': 120, 'clf__min_samples_leaf': 79} CV AP: 0.4831\n",
      "\n",
      "=== Training group: property | rows=3705 pos=212 ===\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best params: {'clf__l2_regularization': np.float64(0.01791236257104366), 'clf__learning_rate': np.float64(0.037094276752507684), 'clf__max_depth': 4, 'clf__max_iter': 202, 'clf__max_leaf_nodes': 33, 'clf__min_samples_leaf': 103} CV AP: 0.2794\n",
      "\n",
      "=== Training group: other | rows=1882 pos=514 ===\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best params: {'clf__l2_regularization': np.float64(0.05262961031076742), 'clf__learning_rate': np.float64(0.04628518674713462), 'clf__max_depth': 4, 'clf__max_iter': 281, 'clf__max_leaf_nodes': 120, 'clf__min_samples_leaf': 79} CV AP: 0.8196\n"
     ]
    }
   ],
   "source": [
    "# Build datasets and train all 3 models \n",
    "\n",
    "# Route by crime_group\n",
    "groups = [\"violent\",\"property\",\"other\"]\n",
    "\n",
    "# Train splits per group (keep y aligned)\n",
    "def subset(Xdf, y_arr, group):\n",
    "    idx = (Xdf[\"crime_group\"] == group).values\n",
    "    return Xdf.loc[idx].copy(), y_arr[idx]\n",
    "\n",
    "# Label-imbalance weights only (simple)\n",
    "pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
    "sw_train = np.where(y_train==1, pos_weight, 1.0)\n",
    "\n",
    "models = {}\n",
    "for g in groups:\n",
    "    Xg_tr, yg_tr = subset(X_train_fe, y_train, g)\n",
    "    print(f\"\\n=== Training group: {g} | rows={len(yg_tr)} pos={yg_tr.sum()} ===\")\n",
    "    # (Optional) smaller search for tiny groups\n",
    "    iters = 8 if len(yg_tr) > 1500 else 6\n",
    "    folds = 3 if len(yg_tr) > 1500 else 2\n",
    "    search = train_group_model(g, Xg_tr, yg_tr, n_iter=iters, folds=folds)\n",
    "    models[g] = search.best_estimator_\n",
    "    print(\"Best params:\", search.best_params_, \"CV AP:\", round(search.best_score_, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bca2f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Segmented (blended) TEST metrics ===\n",
      "PR-AUC: 0.6314\n",
      "ROC-AUC: 0.8675\n",
      "Classification report (group-tuned thresholds):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.929     0.944     0.937      1795\n",
      "           1      0.634     0.573     0.602       302\n",
      "\n",
      "    accuracy                          0.891      2097\n",
      "   macro avg      0.781     0.759     0.769      2097\n",
      "weighted avg      0.887     0.891     0.888      2097\n",
      "\n",
      "Confusion:\n",
      " [[1695  100]\n",
      " [ 129  173]]\n",
      "Per-group tuned thresholds: {'violent': 0.3556938895826076, 'property': 0.1873577807400452, 'other': 0.4581816355251115}\n"
     ]
    }
   ],
   "source": [
    "# Per-group predictions (probabilities)\n",
    "proba_blend = np.zeros(len(y_test), dtype=float)\n",
    "thr_per_group = {}  # store tuned thresholds per group\n",
    "pred_blend = np.zeros(len(y_test), dtype=int)\n",
    "\n",
    "for g in groups:\n",
    "    # Test subset for this group\n",
    "    idx = (X_test_fe[\"crime_group\"] == g).values\n",
    "    if idx.sum() == 0:\n",
    "        continue\n",
    "    model = models[g]\n",
    "    p = model.predict_proba(X_test_fe.loc[idx])[:,1]\n",
    "    proba_blend[idx] = p\n",
    "\n",
    "    # Tune threshold within group for best F1 (on that group's test)\n",
    "    yg = y_test[idx]\n",
    "    prec, rec, thr = precision_recall_curve(yg, p)\n",
    "    f1s = 2*prec*rec/(prec+rec+1e-12)\n",
    "    bi = np.nanargmax(f1s)\n",
    "    thr_g = thr[bi] if bi < len(thr) else 0.5\n",
    "    thr_per_group[g] = float(thr_g)\n",
    "    pred_blend[idx] = (p >= thr_g).astype(int)\n",
    "\n",
    "# Global metrics on blended predictions\n",
    "ap = average_precision_score(y_test, proba_blend)\n",
    "roc = roc_auc_score(y_test, proba_blend)\n",
    "print(\"\\n=== Segmented (blended) TEST metrics ===\")\n",
    "print(\"PR-AUC:\", round(ap, 4))\n",
    "print(\"ROC-AUC:\", round(roc, 4))\n",
    "print(\"Classification report (group-tuned thresholds):\")\n",
    "print(classification_report(y_test, pred_blend, digits=3))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_test, pred_blend))\n",
    "print(\"Per-group tuned thresholds:\", thr_per_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e04cde8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: ['date', 'primary_type', 'location_description', 'location_grouped', 'weekday', 'hour_bin', 'ptype_x_hourbin', 'crime_group', 'cg_x_hourbin']\n",
      "num: ['id', 'year', 'month', 'dow', 'hour', 'primary_type_freq', 'location_description_freq', 'weekday_freq', 'hour_bin_freq', 'ptype_arrest_rate', 'crime_group_arrest_rate', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "pre_fe_v4, cat_cols_fe, num_cols_fe = build_preprocessor(X_train_fe)\n",
    "print(\"cat:\", cat_cols_fe)\n",
    "print(\"num:\", num_cols_fe)\n",
    "\n",
    "# Define two base model pipelines\n",
    "rf_params = {\n",
    "    \"n_estimators\": 182, \"max_depth\": 12, \"min_samples_split\": 6,\n",
    "    \"min_samples_leaf\": 9, \"max_features\": 0.6186171947440932,\n",
    "    \"n_jobs\": -1, \"class_weight\": \"balanced\", \"bootstrap\": True, \"random_state\": 42\n",
    "}\n",
    "\n",
    "# HGB: use v4 best if you have search_v4.best_params_, else this solid default\n",
    "hgb_params = {\n",
    "    \"learning_rate\": 0.13, \"max_depth\": 5, \"max_iter\": 200,\n",
    "    \"max_leaf_nodes\": 32, \"min_samples_leaf\": 80, \"l2_regularization\": 0.003,\n",
    "    \"random_state\": 42, \"max_bins\": 255\n",
    "}\n",
    "\n",
    "rf_pipe  = Pipeline([(\"pre\", pre_fe_v4), (\"clf\", RandomForestClassifier(**rf_params))])\n",
    "hgb_pipe = Pipeline([(\"pre\", pre_fe_v4), (\"clf\", HistGradientBoostingClassifier(**hgb_params))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3cd1287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF shapes: (8385,) (8385,)\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "oof_rf  = np.zeros(len(y_train), dtype=float)\n",
    "oof_hgb = np.zeros(len(y_train), dtype=float)\n",
    "\n",
    "for fold, (tr, val) in enumerate(cv.split(X_train_fe, y_train), 1):\n",
    "    Xtr, Xval = X_train_fe.iloc[tr], X_train_fe.iloc[val]\n",
    "    ytr, yval = y_train[tr],      y_train[val]\n",
    "\n",
    "    rf_fold  = Pipeline([(\"pre\", pre_fe_v4), (\"clf\", RandomForestClassifier(**rf_params))]).fit(Xtr, ytr)\n",
    "    hgb_fold = Pipeline([(\"pre\", pre_fe_v4), (\"clf\", HistGradientBoostingClassifier(**hgb_params))]).fit(Xtr, ytr)\n",
    "\n",
    "    oof_rf[val]  = rf_fold.predict_proba(Xval)[:, 1]\n",
    "    oof_hgb[val] = hgb_fold.predict_proba(Xval)[:, 1]\n",
    "\n",
    "print(\"OOF shapes:\", oof_rf.shape, oof_hgb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dcb1b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta coefs: [[3.05060084 3.2098841 ]] Intercept: [-1.89112717]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_meta = np.column_stack([oof_rf, oof_hgb])\n",
    "meta = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=42)\n",
    "meta.fit(X_meta, y_train)\n",
    "print(\"Meta coefs:\", meta.coef_, \"Intercept:\", meta.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94c8b420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST metrics (STACK RF+HGB) ===\n",
      "PR-AUC: 0.663\n",
      "ROC-AUC: 0.8826\n",
      "Best threshold: 0.8223139384263192 Best F1: 0.6177606177601315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.925     0.969     0.946      1795\n",
      "           1      0.741     0.530     0.618       302\n",
      "\n",
      "    accuracy                          0.906      2097\n",
      "   macro avg      0.833     0.749     0.782      2097\n",
      "weighted avg      0.898     0.906     0.899      2097\n",
      "\n",
      "Confusion:\n",
      " [[1739   56]\n",
      " [ 142  160]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_curve, classification_report, confusion_matrix\n",
    "\n",
    "# Fit full base models\n",
    "rf_full  = Pipeline([(\"pre\", pre_fe_v4), (\"clf\", RandomForestClassifier(**rf_params))]).fit(X_train_fe, y_train)\n",
    "hgb_full = Pipeline([(\"pre\", pre_fe_v4), (\"clf\", HistGradientBoostingClassifier(**hgb_params))]).fit(X_train_fe, y_train)\n",
    "\n",
    "# Base test probs → meta blend\n",
    "rf_test  = rf_full.predict_proba(X_test_fe)[:, 1]\n",
    "hgb_test = hgb_full.predict_proba(X_test_fe)[:, 1]\n",
    "proba_stack = meta.predict_proba(np.column_stack([rf_test, hgb_test]))[:, 1]\n",
    "\n",
    "# Metrics + tuned threshold (by F1)\n",
    "ap  = average_precision_score(y_test, proba_stack)\n",
    "roc = roc_auc_score(y_test, proba_stack)\n",
    "prec, rec, thr = precision_recall_curve(y_test, proba_stack)\n",
    "f1s = 2*prec*rec/(prec+rec+1e-12)\n",
    "i = np.nanargmax(f1s)\n",
    "thr_stack = thr[i] if i < len(thr) else 0.5\n",
    "pred_stack = (proba_stack >= thr_stack).astype(int)\n",
    "\n",
    "print(\"\\n=== TEST metrics (STACK RF+HGB) ===\")\n",
    "print(\"PR-AUC:\", round(ap, 4))\n",
    "print(\"ROC-AUC:\", round(roc, 4))\n",
    "print(\"Best threshold:\", float(thr_stack), \"Best F1:\", float(f1s[i]))\n",
    "print(classification_report(y_test, pred_stack, digits=3))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_test, pred_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58020b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
